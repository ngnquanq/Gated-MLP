{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from random import randrange\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Spatial Gating Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(self, d_ffn, \n",
    "                 seq_len, weight_value=0.05):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_ffn//2)\n",
    "        \n",
    "        # Setup weight for the spatial projection\n",
    "        self.weight = nn.Parameter(torch.zeros(seq_len,seq_len))\n",
    "        nn.init.uniform_(self.weight, a=-weight_value, b=weight_value)\n",
    "        \n",
    "        # Setup bias for the spatial projection\n",
    "        self.bias = nn.Parameter(torch.ones(seq_len))\n",
    "\n",
    "    def forward(self, x):\n",
    "        u, v = x.chunk(2, dim=-1)\n",
    "        v = self.norm(v)\n",
    "        \n",
    "        weight, bias = self.weight, self.bias\n",
    "        v = einsum('b n d, m n -> b m d', v, weight) + rearrange(bias, 'n -> () n ()')\n",
    "        return u * v\n",
    "    \n",
    "batch_size = 32\n",
    "d_ffn = 1024\n",
    "seq_len = 128\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_ffn)\n",
    "sgu = SpatialGatingUnit(d_ffn, seq_len)\n",
    "y = sgu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 512])\n",
      "torch.Size([32, 128, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug gMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 512])\n",
      "torch.Size([32, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "class gMLPBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_ffn, seq_len):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.channel_proj_U = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ffn),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.sgu = SpatialGatingUnit(d_ffn, seq_len)\n",
    "        self.channel_proj_V = nn.Sequential(\n",
    "            nn.Linear(d_ffn//2, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.norm(x)\n",
    "        x = self.channel_proj_U(x)\n",
    "        x = self.sgu(x)\n",
    "        x = self.channel_proj_V(x)\n",
    "        return x + res\n",
    "    \n",
    "d_model = 512\n",
    "d_ffn = 1024\n",
    "seq_len = 128\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "block = gMLPBlock(d_model, d_ffn, seq_len)\n",
    "y = block(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug gMLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class gMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ffn, seq_len, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.Embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(gMLPBlock(d_model, d_ffn, seq_len))\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "        return x\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "d_ffn = 1024\n",
    "seq_len = 128\n",
    "num_layers = 6\n",
    "num_classes = 2\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "model = gMLP(vocab_size, d_model, d_ffn, seq_len, num_layers, num_classes)\n",
    "y = model(x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
